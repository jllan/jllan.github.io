---
title: 梯度下降法和牛顿法
date: 2017-04-23 13:47:36
categories: IT技术
tags: 机器学习
---
# 使用梯度下降法和牛顿法求解最优解

## 问题描述

$ A = \begin{bmatrix} 4 & 2 \\ 1 & 3 \\ \end{bmatrix} $,
$ B = \begin{bmatrix} 3 \\ 2 \\ \end{bmatrix} $

Ax=B，求x

## 思路
1. 梯度下降法
![深度截图20170423135233.png](http://upload-images.jianshu.io/upload_images/1713353-910f02e76c5eee83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
2. 牛顿法
![深度截图20170423135205.png](http://upload-images.jianshu.io/upload_images/1713353-81780540ee2f7ac6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![深度截图20170423135217.png](http://upload-images.jianshu.io/upload_images/1713353-3dd7e16d374e482c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 求解过程
```python
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

# numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)创建num个start～stop之间的等差数列
# meshgrid()产生格点矩阵，用于三维曲面的分格线座标
# meshgrid(range(-3,4),range(-2,3))产生两个都是5*7的矩阵X和Y,其中X是由第一个行向量产生，Y是由第二个行向量产生
X, Y = np.meshgrid(np.linspace(-5, 10, 100), np.linspace(-5, 10, 100))

A = np.matrix([[4.0,2.0],[1.0,3.0]])
B = np.matrix([[3.0],[2.0]])

R = A.T.dot(A)
P = A.T.dot(B)

print(R)
print(P)
print(B.T.dot(B))

# 定义代价方程
# 不明白这个代价方程怎么来的
def cost_func(X,Y):
    return R[0,0]*(X**2)+R[1,1]*(Y**2)+(R[0,1]+R[1,0])*X*Y-2*P[0,0]*X-2*P[1,0]*Y

# 如果这样定义代价方程，计算出的结果有些问题，暂时还没搞明白
def cost_func2(X):
    return X.T.dot(R).dot(X) - X.T.dot(P) - P.T.dot(X) + B.T.dot(B)

z = cost_func(X,Y)
```

    [[ 17.  11.]
     [ 11.  13.]]
    [[ 14.]
     [ 12.]]
    [[ 13.]]



```python
# w, w_n分别用来存最速下降和牛顿法的迭代过程
w=np.matrix([[0],[-4]])
w_n=np.matrix([[0],[-4]])

# 梯度下降法
for i in range(100):
    w_t = w[:,-1]
    w=np.hstack((w,w_t-0.01*(R.dot(w_t)-P)))

# NewTon法 

#Hessian matrix，这里不明白    
H=np.matrix([[2.0*R[0,0],R[0,1]+R[1,0]],[R[0,1]+R[1,0],2.0*R[1,1]]])

for _ in range(30):
    w_t = w_n[:,-1]
    w_n=np.hstack((w_n,w_t-H.I.dot(R.dot(w_t)-P)))
    
w = np.array(w)
w_n = np.array(w_n)

# 学习曲线
L=cost_func(w[0,:], w[1,:])
L_n=cost_func(w_n[0,:], w_n[1,:])


# 输出图形
# 2d plot

f, (ax1, ax2) = plt.subplots(1, 2)
CS = ax1.contour(X, Y, z)
ax1.plot(w[0,:], w[1,:], 'bo',w_n[0,:],w_n[1,:], 'ro')

ax1.clabel(CS, inline=1, fontsize=10)
ax1.set_title('contour')
ax1.set_xlabel('w0')
ax1.set_ylabel('w1')
ax1.legend(('grad', 'Newton'))
ax1.grid(True)


ax2.plot(range(101),L,'b',range(31),L_n,'r')
ax2.set_title('cost learn curve')
ax2.set_ylabel('cost')
ax2.legend(('grad', 'Newton'))
ax2.grid(True)


# 3d plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# 先画出整个大曲面，然后在大曲面上画出迭代过程中的每个点
ax.plot_surface(X,Y, z, rstride=4, cstride=4, color='b')
ax.plot(w[0,:],w[1,:],L,'ro')
ax.plot(w_n[0,:],w_n[1,:],L_n,'go')
ax.legend(('grad', 'Newton'))

plt.show()
```


![output_4_0.png](http://upload-images.jianshu.io/upload_images/1713353-afbffbcccc495767.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![output_4_1.png](http://upload-images.jianshu.io/upload_images/1713353-dd048c7e0fa8030f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)