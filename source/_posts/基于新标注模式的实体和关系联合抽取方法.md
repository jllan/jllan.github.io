---
title: 基于新标注模式的实体和关系联合抽取方法
date: 2017-07-09 13:31:27
tags: nlp
categories: IT技术
---
## 摘要
联合抽取实体和关系是信息抽取中的一项重要任务。为了解决这一问题，我们首次提出了一种能够把联合抽取任务转换为标注问题的新标注模式。然后，基于新标注模式，我们学习不同的end-to-end的模型来直接抽取实体和实体关系，而不需要分开来识别实体和关系。我们用远程监督方法在公共数据集上做实验，实验结果证明这种基于标注的方法比大多数已存在的串行式和联合式的学习方法要好。甚至，论文中提出的这种end-to-end的模型在公共数据集熵取得了最好的结果。

## 1 介绍
联合抽取实体和关系是为了从非结构化的文本中同时识别命名实体和实体之间的语义关系。不同于其关系词是从给定的句子中进行抽取的开放信息抽取（Open IE），在联合抽取任务中，关系词是从可能没在给出的句子中出现的预定义好的关系集合中抽取的。这在知识抽取和知识库构建中是一个很重要的问题。
传统的处理这种处理实体和关系抽取的方法是串联式的。即先抽取实体，然后识别实体间的关系。这种分开来处理的方式比较简单，而且各个模块都比较灵活。但是这种方法忽视了两个任务之间的联系，实体识别的结果可能会影响关系的抽取。
![图1](http://upload-images.jianshu.io/upload_images/1713353-cecbb0e305a4e3b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
不同于串联式的方法，联合抽取是用一个模型来抽取实体和关系。这种方式可以有效的整合实体和关系信息，并且达到一个很好的效果。然而，现有的联合抽取方法大多都是基于特征来实现的，并且非常依赖其他可能会引入误差的NLP工具。为了减少人工抽取特征工作，提出基于神经网络的end-to-end模型来联合抽取实体和关系。尽管这些模型能够在同一个模型当中共享实体抽取和关系抽取的参数，但是实体和关系抽取是分开进行的而且容易产生冗余信息。例如图一中的句子包含三个实体：“美国”，“特朗普”和“苹果公司”。但是只有“美国”和“特朗普”之间有一个“国家总统”的关系。实体“苹果公司”与其它实体之间没有明显的关系。因此，这句话的抽取结果应该是{美国， 国家-总统， 特朗普}，我们称之为三元组。
在这篇论文中，我们的研究点在于由两个实体以及两个实体间关系组成的三元组的抽取。因此，我们直接对三元组进行建模，而不是分别对实体和关系进行建模。基于此，我们提出了标注模式和end-to-end模型来处理这个问题。我们设计了一种新的包含了实体和关系的标签。基于这种标注模式，把联合抽取任务转换为标注问题。通过这种方法，我们能够简单地使用神经网络来建模而无需复杂的特征工程。
最近，基于LSTM的end-to-end模型已经被成功运用到各种标注任务，如命名实体识别等。LSTM能够学习长词，这对句子模型很有好处。因此，基于新的标注方法，我们学习用不同的end-to-end模型来解决问题。为了适应特殊标签，我们还通过增加一个偏置损失函数来修改解码方法。
我们提出的方法是一个监督学习算法。实际上，手工标注包含大量实体和关系的数据集的这一过程是很花费时间并且容易出错的。因此，我们在公开数据集来进行我们的实验。实验结果证明我们的标注模式是有效的。另外，我们的end-to-end模型在公开数据集熵达到了最好的效果。
这篇论文的主要贡献在于：
（1）提出了新的标注方法，可以把联合抽取实体和关系的任务转换为标注任务。（2）基于新的标注方法，我们学习用不同的end-to-end模型来解决问题。（3）对于end-to-end模型增加了偏置损失函数，增强了相关实体之间的联系。

## 2 相关工作
识别和关系抽取是构建知识图谱的重要一步，对很多NLP任务都有好处。主要有两种方法广泛的被应用与实体和关系的抽取任务，一种是串联式的，另一种是联合式的。
串联式方法把这一任务分解为两个不同的子任务，既命名实体识别（NER）和关系分类（RC）。传统的NER模型是线性统计模型，例如隐马尔科夫（HMM）模型和条件随机场（CRF）模型。最近一些神经网络结构也被成功应用到NER中，这被当做是序列标注任务。已有的RC方法也可以被分为基于手工构造特征的方法和神经网络方法。
联合抽取实体和关系使用的是单个模型。大多数联合式方法是基于特征来实现的，最近使用基于LSTM的模型能够减少人工工作。
不同于以上的方法，这篇论文中提出的方法是基于特殊标注方式的，因此我们使用end-to-end模型而不需要NER和RC。End-to-end模型是把输入句子映射一个有意义向量中然后再生成一个序列。它被广泛应用于机器翻译和序列标注任务。大多数方法都使用双向LSTM对输入句子进行编码，但是解码方法总是不同的。例如，使用一个CRF层解码标签序列，同时应用LSTM层来产生标签序列。
## 3 方法
我们提出了一种带有偏置损失函数的新标注模式来联合抽取实体和关系。这一部分，我们首先介绍如何把抽取问题转换为标注问题，然后详述该模型。
![图2](http://upload-images.jianshu.io/upload_images/1713353-b54d59fbfa977bb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
### 3.1 标注模型
图2是如何标记结果的示例。为每个单词分配一个有助于提取结果的标签，标签“O”表示“其他”标签，这意味着对应的单词与提取的结果无关。除了“O”之外，其他标签由三部分组成：实体中的单词位置，关系类型和关系角色。我们使用“BIES”（开始，内部，结束，单个）标志来表示实体中单词的位置信息。关系类型信息是从一组预定的关系中获得的，并且关系角色信息由数字“1”和“2”表示。提取的结果由三元组表示：（Entity1，RelationType，Entity2）。 “1”表示该词属于三元组中的第一个实体，而“2”属于关系类型后面的第二个实体。因此，标签的总数为$N_{t} = 2 * 4 * | R | + 1$，其中| R |是预定义关系集的大小。图2是说明我们的标记方法的示例。 输入句子包含两个三元组：{美国，国家-总统，特朗普}和{苹果公司，公司-创始人，史蒂文·保罗·乔布斯，其中“国家总统”和“公司创始人”是预定义的关联类型。 “联合”，“国家”，“特朗普”，“苹果”，“公司”，“史蒂文”，“保罗”和“乔布斯”都与最终提取的结果相关。 因此，它们是基于我们的特殊标签来标记的。 例如，“联合”这个词是“美国”这个实体的第一个词，与“国家总统”有关，所以它的标签是“B-CP-1”。 对应于“美国”的另一个实体“特朗普”被标记为“S-CP-2”。 另一方面，与最终结果无关的其他词语标记为“O”。

### 3.2 从标签序列抽取结果
从图2的标签序列，我们知道“特朗普”和“美国”共享相同的关系类型“国家-总统”，“苹果公司”和“史蒂文·保罗·乔布斯”共享相同的关系类型“公司-创始人”。 我们将具有相同关系类型的实体组合成三元组以获得最终结果。 因此，“特朗普”和“美国”可以组合成一个关系类型是“国家总统”的三元组。 因为“特朗普”的关系角色是“2”，“美国”是“1”，最终的结果是{美国，国家-总统，特朗普}。 这同样适用于{苹果公司，公司-创始人，史蒂文·保罗·乔布斯}。
此外，如果一个句子包含两个或更多个具有相同关系类型的三元组，那么我们根据最近原则将每两个实体组合成一个三元组。 例如，如果图2中的关系类型“国家总统”是“公司创始人”，则在给定句子中将有四个实体具有相同的关系类型。 “美国”最接近实体“特朗普”，“苹果公司”最接近“乔布斯”，结果将是{美国，公司-创始人，特朗普}和{苹果公司，公司-创始人， 史蒂文·保罗·乔布斯}。
在这篇论文中，我们仅考虑一个实体只属于一个三元组这种情况，把重叠关系识别的任务放到未来的工作中。

### 3.3 End-to-end模型
近年来，基于神经网络的end-to-end模型被广泛应用于序列标签任务中。在本文中，我们调查端到端模型来生成标签序列，如图3所示， 它包含一个用于对输入语句进行编码的双向长短期记忆（Bi-LSTM）层和一个基于LSTM具有偏置损失函数的解码层。偏置损失函数可以增强实体标签的相关性。
#### 3.3.1 双向LSTM编码层
![图3](http://upload-images.jianshu.io/upload_images/1713353-f4622d2d2a2ccde4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在序列标注问题中，Bi-LSTM编码层已经显示了捕获每个单词的语义信息的有效性。它包含了前向lstm层，向后lstm层和连接层。词嵌入层将具有1-hot表示的单词转换为嵌入向量。 因此，一个词序列可以表示为W = {w1，... wt，wt + 1 ... wn}，其中$w_{t}\in R^{d}$是对应于d维字向量中的第t个词，n是给定句子的长度。字嵌入层后，有两个平行的LSTM层：前向LSTM层和后向LSTM层。 LSTM体系结构由一组循环连接的子网组成，称为内存块。 每个时间步长是一个LSTM内存块。Bi-LSTM编码层中的LSTM存储块用于基于先前的隐藏向量$h_{t-1}$，先前的小区向量$c_{t-1}$和当前输入字嵌入$w_{t}$来计算当前隐藏向量$h_{t}$。其结构图如图3所示，详细操作定义如下：
![](http://upload-images.jianshu.io/upload_images/1713353-ca5c5d5339105f8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中i，f和o分别是输入门，忘记门和输出门，b是偏置项，c是单元存储器，W(.)是参数。 对于每个单词$w_{t}$，前向LSTM层将通过考虑从$w_{1}$到$w_{0}$的上下文信息来编码$w_{t}$，其被标记为![](http://upload-images.jianshu.io/upload_images/1713353-4187b2a73f2ab1d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)。 以类似的方式，后向LSTM层将基于从$w_{n}$到$w_{t}$的上下文信息来编码$w_{t}$，其被标记为![](http://upload-images.jianshu.io/upload_images/1713353-0d2a47ff4ef7c830.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)。最后，我们连接![](http://upload-images.jianshu.io/upload_images/1713353-0d2a47ff4ef7c830.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 和 
![](http://upload-images.jianshu.io/upload_images/1713353-4187b2a73f2ab1d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
来表示t的编码信息，表示为
![](http://upload-images.jianshu.io/upload_images/1713353-a5802b7288c70a58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)。
#### 3.3.2 LSTM解码层
我们还采用LSTM结构来生成标签序列。当检测到单词$w_{t}$的标签时，解码层的输入是：从Bi-LSTM编码层得到的$h_{t}$，前一个预测标签嵌入$T_{t-1}$，前一个单元值${c_{t-1}}^{(2)}$，前一个解码层的隐藏向量${h_{t-1}}^{(2)}$。 $LSTM_{d}$内存块的结构图如图3(c)所示，详细操作定义如下：
![](http://upload-images.jianshu.io/upload_images/1713353-5cab43c31b873351.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
最后通过softmax计算基于标签预测向量$T_{t}$计算归一化的实体标签概率：
![](http://upload-images.jianshu.io/upload_images/1713353-d2194e3e18625d99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中$W_{y}$是softmax矩阵，$N_{t}$是标签数量。因为T类似于标签嵌入，LSTM能够学习长期依赖性，因此解码方式可以建立标签交互。

####  3.3.3 偏置目标函数
我们训练我们的模型以最大化数据的对数似然，我们使用的优化方法是Hinton在2012年中提出的RMSprop。 目标函数可以定义为：
![](http://upload-images.jianshu.io/upload_images/1713353-8e7540fec5e090a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中|D|是训练集的大小，$L_{j}$是句子$x_{j}$的长度，${y_{t}}^{(j)}$是句子$x_{j}$中词t的标签，${p_{t}}^{(j)}$是公式15中定义好的标签的归一化概率。此外，I(O)是一个切换函数，用于区分可以指示结果的标签“O”和关系标签的损失量。 定义如下：
![](http://upload-images.jianshu.io/upload_images/1713353-899b59c1b0b3d622.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
α是偏置权重。α越大，其对模型中关系标签的影响越大。

## 4 实验
### 4.1 实验设置
#### 4.1.1 数据集
为了评估我们的方法的性能，我们使用由远程监督方法生成的公共数据集NYT2（Ren et al.,2017）。通过远程监控方法可以获得大量的培训数据，无需手动标注，虽然手动标注测试集可以确保其质量。 总共训练数据包含353000个三元组，测试集包含3880个三元组。 此外，关系集的大小是24。
#### 4.1.2 评估
我们采用标准精度(Prec)，召回(Rec)和F1值来评估结果。与传统方法不同，我们的方法可以提取三元组而不需要知道实体类型信息。 换句话说，我们没有使用实体类型的标签来训练模型，因此我们不需要在评估中考虑实体类型。当它的关系类型和两个对应实体的头部偏移都是正确时，三元组被认为是正确的。此外，如Ren等人所做的，提出了ground-truth关系并且排除“None”标签。我们通过从测试集中随机抽取10％的数据创建一个验证集，并使用剩余的数据作为评估。 我们对每个实验运行10次，然后报告平均结果及其标准差，如表1所示。
![表1](http://upload-images.jianshu.io/upload_images/1713353-a0b5597fdcb90f56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
#### 4.1.3 超参数
我们的模型由Bi-LSTM编码层和具有偏置目标函数的LSTM解码层组成。在编码部分中使用的词嵌入是通过在NYT训练语料库上运行word2vec来初始化的。词嵌入的维数为d = 300。我们使用嵌入层上的缺失来校准我们的网络，并且丢弃率是0.5。编码层的lstm单位数为300，解码层数为600。与表1的结果对应的偏置参数α为10。
#### 4.1.4 基准线
我们将我们的方法与几种传统的三元组提取方法进行比较，可以分为以下几类：串联式方法，联合提取方法和基于我们的标记方案的端到端方法。
对于串联式方法，我们遵循Ren等人2017年的设置：NER结果由CoType获得，然后应用几种经典关系分类方法来检测关系。这些方法是：（1）DS-逻辑回归是一种远程监督和基于特征的方法，其结合了监督IE和无监督IE特征的优点; （2）LINE是一种网络嵌入方式，适用于任意类型的信息网络; （3）FCM是一种组合模型，它将词汇化语言语境和词汇嵌入结合起来进行关联抽取。
本文使用的联合提取方法如下：（4）DS-Joint是一种监督方法，它使用结构感知机在人工标注的数据集合上提取实体和关系; （5）MultiR是基于多实例学习算法的典型的远程监督方法，用于对抗有噪点的训练数据; （6）Co-Type是一个独立领域的框架，它将实体，关系，文本特征和类型标签共同嵌入到有意义的表示中。
此外，我们也比较了我们的方法和两种传统的end-to-end标注模型方法：LSTM-CRF和LSTM-LSTM。LSTM-CRF用于实体识别，它是通过使用双向L-STM编码输入句和条件随机字段来预测实体标签序列。 与LSTM-CRF不同，LSTM-LSTM使用LSTM层解码标签序列而不是CRF。这两种方法是首次用基于我们的标签方案的联合提取实体和关系的方法。

### 4.2 实验结果
从表1可以看出，我们的LSTM-LSTM-Bias方法在F1值的表现上优于所有其它方法，并且比最好方法CoType有3％的提高。这显示了我们提出的方法的有效性。 此外，从表1可以看出，联合提取方法优于串联式方法，标注方法比大多数联合提取方法更好。这也验证了我们的标注模式对共同提取实体和关系的任务的有效性。
与传统方法相比，端对端模型的精度有显着提高。但只有LSTM-LSTM-Bias才能更好地平衡精度和召回。原因可能是这些端对端模型都使用Bi-LSTM编码输入句和不同的神经网络来解码结果。基于神经网络的方法可以很好地适应数据。 因此，他们可以很好地学习训练集的共同特征，并可能导致较低的可扩展性。我们也发现LSTM-LSTM模型要优于LSTM-CRF模型。因为LSTM能够学习长期的依赖关系，CRF很好地捕捉整个标签序列的联合概率。 相关标签可能彼此间距很远。 因此，LSTM的解码方式要好于CRF。 LSTM-LSTM-Bias增加了偏差权重以增强实体标签的影响，并削弱无效标签的影响。 因此，在这种标注方案中，我们的方法可以比普通的LSTM解码方法更好。

## 5 分析和讨论
### 5.1 错误分析
![表2](http://upload-images.jianshu.io/upload_images/1713353-5daf184265d01e0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在本文中，我们着重于提取由两个实体和一个关系组成的三元组。 表1显示了预测结果，只有当两个对应的实体的关系类型和头部偏移都是正确的时，找到的三元组才是是正确的。为了找出影响端对端模型结果的因素，我们分析了预测三元组中每个元素的性能，如表2所示。E1和E2分别代表每个实体的预测性能。如果第一个实体的头部偏移正确，则E1的实例是正确的，E2也是如此。无论关系类型如何，如果两个对应的实体的头部偏移都是正确的，则（E1，E2）的实例是正确的。
如表2所示，与E1和E2相比，（E1，E2）的精度更高。但其召回率比E1和E2低。这意味着一些预测实体不会形成一对。他们只得到E1，找不到对应的E2，或获得E2，找不到对应的E1。因此，它导致对更多单个E和更少（E1，E2）对的预测。 因此，实体对（E1，E2）具有比单个E更高的精度和更低的召回率。此外，与表1中的预测结果相比，表2中的（E1，E2）的预测结果具有约3％的改善，这意味着3%的测试数据预测结果是错误的，因为预测关系类型是错误的。
### 5.2 偏置损失分析
![图4](http://upload-images.jianshu.io/upload_images/1713353-f460934320bd1998.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
与LSTM-CRF和LSTM-LSTM不同，我们的方法偏向于关系标签，以增强实体之间的链接。 为了进一步分析偏置目标函数的影响，我们可以从图4中看出每个端对端方法预测单个实体的比例。单个实体是指找不到相应实体的实体。无论图4显示的是E1还是E2，我们的方法都可以在单个实体上获得相对较低的比例。这意味着当比较LSTM-CRF和LSTM-LSTM时，我们的方法可以有效地关联两个实体，而不关心关系标签。
此外，我们调整偏置参数α从1到20，相应的预测结果如图5所示。如果α太大，会影响预测的准确性，如果α太小，则召回率将下降。 当α= 10时，LSTM-LSTM-Bias可以平衡准确率和召回率，获得最佳的F1值。
![图5](http://upload-images.jianshu.io/upload_images/1713353-09c19b6807009cb6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
### 5.3 案例分析
在本节中，我们观察了端到端方法的预测结果，然后选择几个代表性的例子来说明方法的优缺点，如表3所示。每个示例包含三行，第一行是黄金标准，第二行和第三行分别是LSTM-LSTM和LSTM-LSTM-Bias模型的提取结果。
S1表示两个相互关联的实体之间的距离越彼此远离，就越难以发现其关系。与LSTM-LSTM相比，LSTM-LSTM-Bias使用偏差目标函数，增强实体之间的相关性。 因此，在这个例子中，LSTM-LSTM-Bias可以提取两个相关实体，而LSTM-LSTM只能提取一个“Florida”实体，不能检测到“Panama City Beach”。
S2是一个负面例子，显示这些方法可能会错误地预测一个实体。 Nuremberg和Germany之间没有任何指示性的词汇。 此外，Germany和MiddleAges之间的“a * of *”这种模式可能容易错误的导致模型认为它们之间存在“包含”关系。可以通过将这种表达模式的一些样本添加到训练集中来解决该问题。
S3是一个模型可以预测实体的头部偏移量，但是关系角色是错误的例子。 LSTM-LSTM将“Stephen A. Schwarzman”和“Blackstone Group”视为实体E1，找不到相应的E2。 虽然LSTM-LSMT-Bias可以找到实体对（E1，E2），但它颠倒了“Stephen A. Schwarzman”和“Blackstone Group”的角色。 这表明LSTM-LSTM-Bias能够更好地预测实体对，但在区分两个实体之间的关系方面仍有待改进。

## 6 结论
在本文中，我们提出了一种新颖的标注方案，并研究了端对端模型共同提取实体和关系。实验结果表明我们提出的方法的有效性。但是，重叠关系的识别仍然存在缺陷。 在未来的工作中，我们将使用多个分类器替换输出层中的softmax函数，以便一个单词可以有多个标签。 这样一来，一个字可以出现在多个三元组结果中，这可以解决重叠关系的问题。虽然，我们的模型可以增强实体标签的效果，但两个相应实体之间的关联仍然需要在下一个工作中进行细化。

## 致谢
感谢Xiang Ren的数据集细节和有益的讨论。 该工作也得到了中国国家高技术研究发展计划（863计划）（授权号：2015AA015402），国家自然科学基金（61602479）和国家自然科学基金项目（61501463）的支持。

## 参考文献
Michele Banko, Michael J Cafarella, Stephen Soder- land, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJ- CAI. volume 7, pages 2670–2676. 
Jason PC Chiu and Eric Nichols. 2015. Named enti- ty recognition with bidirectional lstm-cnns. In Pro- cessings of Transactions of the Association for Com- putational Linguistics. 
Cıcero Nogueira et al. dos Santos. 2015. Classifying relations by ranking with convolutional neural net- works. In Proceedings of the 53th ACL internation- al conference. volume 1, pages 626–634. Matthew R Gormley, Mo Yu, and Mark Dredze. 2015. Improved relation extraction with feature-rich com- positional embedding models. In Proceedings of the EMNLP. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735–1780.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge- based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computation- al Linguistics. Association for Computational Lin- guistics, pages 541–550. 
Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991 . 
Nal Kalchbrenner and Phil Blunsom. 2013. Recurren- t continuous translation models. In EMNLP. vol- ume 3, page 413. 
Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy mod- els for extracting relations. In Proceedings of the 43th ACL international conference. page 22. Arzoo Katiyar and Claire Cardie. 2016. Investigating lstms for joint extraction of opinion entities and rela- tions. In Proceedings of the 54th ACL international conference.
John Lafferty, Andrew McCallum, Fernando Pereira, et al. 2001. Conditional random fields: Probabilis- tic models for segmenting and labeling sequence da- ta. In Proceedings of the eighteenth international conference on machine learning, ICML. volume 1, pages 282–289. 
Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the NAACL international confer- ence. 
Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proceedings of the 52rd Annual Meeting of the Association for Computational Linguistics. pages 402–412. Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za- iqing Nie. 2015. Joint entity recognition and disam- biguation. In Conference on Empirical Methods in Natural Language Processing. pages 879–888.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems. pages 3111–3119. 
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju- rafsky. 2009. Distant supervision for relation ex- traction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL. Association for Computational Linguistics, pages 1003–1011. 
Makoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using lstms on sequences and tree structures. In Proceedings of the 54rd Annual Meet- ing of the Association for Computational Linguistic- s.
Makoto Miwa and Yutaka Sasaki. 2014. Modeling joint entity and relation extraction with table repre- sentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing. pages 1858–1869. 
David Nadeau and Satoshi Sekine. 2007. A sur- vey of named entity recognition and classification. Lingvisticae Investigationes 30(1):3–26. 
Alexandre Passos, Vineet Kumar, and Andrew McCal- lum. 2014. Lexicon infused phrase embeddings for named entity resolution. In International Confer- ence on Computational Linguistics. pages 78–86. 
Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, Tarek F Abdelzaher, and Jiawei Han. 2017. Cotype: Joint extraction of typed entities and relations with knowledge bases. In Proceedings of the 26th WWW international conference. 
Bryan et al. Rink. 2010. Utd: Classifying semantic re- lations by combining lexical and semantic resources. In Proceedings of the 5th International Workshop on Semantic Evaluation. pages 256–259. Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint infer- ence of entities, relations, and coreference. In Pro- ceedings of the 2013 workshop on Automated knowl- edge base construction. ACM, pages 1–6.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural network- s. In Advances in neural information processing sys- tems. pages 3104–3112. 
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale in- formation network embedding. In Proceedings of the 24th International Conference on World Wide Web. ACM, pages 1067–1077. 
Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop. In COURSERA: Neural networks for machine learning. 
Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. 2016. Supertagging with lstms. In Proceed- ings of the NAACL international conference. pages 232–237. 
Kun et al. Xu. 2015a. Semantic relation classification via convolutional neural networks with simple neg- ative sampling. In Proceedings of the EMNLP. 
Yan et al. Xu. 2015b. Classifying relations via long short term memory networks along shortest depen- dency paths. In Proceedings of EMNLP internation- al conference. 
Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of the 51rd Annual Meeting of the Association for Computational Linguistics. pages 1640–1649.
Xiaofeng Yu and Wai Lam. 2010. Jointly identifying entities and extracting relations in encyclopedia tex- t via a graphical model approach. In Proceedings of the 21th COLING international conference. pages 1399–1407. 
Daojian et al. Zeng. 2014. Relation classification via convolutional deep neural network. In Proceedings of the 25th COLING international conference. pages 2335–2344. 
Feifei Zhai, Saloni Potdar, Bing Xiang, and Bowen Zhou. 2017. Neural models for sequence chunk- ing. In Proceedings of the AAAI international con- ference. 
Suncong Zheng, Jiaming Xu, Peng Zhou, Hongyun Bao, Zhenyu Qi, and Bo Xu. 2016. A neural net- work framework for relation extraction: Learning entity semantic and relation pattern. Knowledge- Based Systems 114:12–23.
